{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 5083,
     "status": "ok",
     "timestamp": 1621780568271,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "DimyJzWZVpoQ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import os\n",
    "import random\n",
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only runs once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ-ExvehbQgU",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# root = os.path.abspath('jpg')\n",
    "# labels = scipy.io.loadmat('imagelabels.mat')['labels'].squeeze()\n",
    "\n",
    "# for cls in range(102):\n",
    "#     cls_folder = os.path.join(root, f'{cls}')\n",
    "#     os.makedirs(cls_folder, exist_ok=True)\n",
    "\n",
    "#     indices = np.argwhere(labels==cls+1).squeeze()\n",
    "    \n",
    "#     for idx in indices:\n",
    "#         s_idx = str(idx+1)\n",
    "\n",
    "#         while len(s_idx) != 5:\n",
    "#             s_idx = '0'+s_idx\n",
    "\n",
    "#         file = os.path.join(root, f'image_{s_idx}.jpg')\n",
    "#         try:\n",
    "#             os.rename(file, os.path.join(cls_folder, f'{s_idx}.jpg'))\n",
    "#         except OSError as e:\n",
    "#             print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1621780570802,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "WlC2TnZeek8O"
   },
   "outputs": [],
   "source": [
    "labels = scipy.io.loadmat('imagelabels.mat')['labels'].squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1621780579817,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "8pMWxfli-DD9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def seed_all(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.random.manual_seed(seed)\n",
    "    \n",
    "def data_map(path):\n",
    "    dmap = {}\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        if files:\n",
    "            k = os.path.split(root)[-1]\n",
    "            k = int(k.split('_')[0])\n",
    "            dmap[k] = np.array([os.path.join(root, f) for f in files])\n",
    "            \n",
    "    return dmap\n",
    "        \n",
    "def train_val_test_split(dmap, p_val=0.25, p_test=0.25):\n",
    "    train, val, test = {}, {}, {}\n",
    "    \n",
    "    for cls, samples in dmap.items():\n",
    "        num_val = int(p_val * len(samples))\n",
    "        num_test = int(p_test * len(samples))\n",
    "        \n",
    "        data = samples.copy()\n",
    "        np.random.shuffle(data)\n",
    "        \n",
    "        val[cls] = data[:num_val].copy()\n",
    "        test[cls] = data[num_val:num_val+num_test].copy()\n",
    "        train[cls] = data[num_val+num_test:].copy()\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "def get_mean_std(dmap):\n",
    "    trns = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize((256, 256)),\n",
    "    ])\n",
    "    \n",
    "    n_samples = sum([len(v) for v in dmap.values()])\n",
    "    images = torch.zeros((n_samples, 3, 256, 256), dtype=torch.float32)\n",
    "    \n",
    "    idx = 0\n",
    "    for samples in dmap.values():\n",
    "        for path in samples:\n",
    "            print(f'\\r{idx+1}/{n_samples}', end='')\n",
    "            img = cv2.imread(path)[:,:,::-1].copy()\n",
    "            img = trns(img)\n",
    "            images[idx] = img\n",
    "            idx += 1\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    images = images.transpose(1, 0).reshape(3, -1)\n",
    "    return {'mean': images.mean(dim=-1), 'std': images.std(dim=-1)}\n",
    "\n",
    "seed_all(42069)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 14990,
     "status": "ok",
     "timestamp": 1621780597715,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "eLrrbg4r-DEA"
   },
   "outputs": [],
   "source": [
    "data = data_map('jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 405,
     "status": "ok",
     "timestamp": 1621780599785,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "u0B3ynRH-DEC"
   },
   "outputs": [],
   "source": [
    "train_map, val_map, test_map = train_val_test_split(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leakage test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = []\n",
    "ts = []\n",
    "vl = []\n",
    "\n",
    "for cl, samp in train_map.items():\n",
    "    tr.extend(list(samp))\n",
    "\n",
    "\n",
    "for cl, samp in test_map.items():\n",
    "    ts.extend(list(samp))\n",
    "    \n",
    "\n",
    "for cl, samp in val_map.items():\n",
    "    vl.extend(list(samp))\n",
    "    \n",
    "len(set(tr) & set(ts)) + len(set(ts) & set(vl)) + len(set(tr) & set(vl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1621780605181,
     "user": {
      "displayName": "Vladi Shtompel",
      "photoUrl": "",
      "userId": "17809558633967063947"
     },
     "user_tz": -180
    },
    "id": "K7SI43rb-DEH"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class ImageDataSet(Dataset):\n",
    "    \n",
    "    def __init__(self, data_map, data_transforms):\n",
    "        self.data = data_map\n",
    "        self.transform = data_transforms\n",
    "        self._len = sum([len(v) for v in self.data.values()])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._len\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        for cls, samples in self.data.items():\n",
    "            \n",
    "            if idx < len(samples):\n",
    "                item = {'X': samples[idx], 'y': cls}\n",
    "                break\n",
    "                \n",
    "            else:\n",
    "                idx -= len(samples)\n",
    "                \n",
    "        else:\n",
    "            raise IndexError(f\"Index {idx} is out of range for dataset with length {self._len}\")\n",
    "        \n",
    "        image = cv2.imread(item['X'])[:,:,::-1].copy()\n",
    "        image = self.transform(image)\n",
    "        label = torch.tensor(item['y'])\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "                                                                                   \n",
    "                                                                                   \n",
    "class NLL_OHEM(nn.NLLLoss):                                                     \n",
    "    \"\"\" Online hard example mining. \n",
    "    Needs input from nn.LogSotmax() \"\"\"                                             \n",
    "                                                                                   \n",
    "    def __init__(self, ratio, *args, **kwargs):      \n",
    "        super(NLL_OHEM, self).__init__(*args, **kwargs)                                 \n",
    "        self.ratio = ratio\n",
    "                                                                                   \n",
    "    def forward(self, x, y, ratio=None):                                           \n",
    "        if ratio is not None:                                                      \n",
    "            self.ratio = ratio        \n",
    "            \n",
    "        num_inst = x.size(0)                                                       \n",
    "        num_hns = int(self.ratio * num_inst)\n",
    "        \n",
    "        x_ = x.clone()    \n",
    "        inst_losses = torch.autograd.Variable(torch.zeros(num_inst)).cuda()      \n",
    "        \n",
    "        for idx, label in enumerate(y.data):                                       \n",
    "            inst_losses[idx] = -x_.data[idx, label]    \n",
    "            \n",
    "        _, idxs = inst_losses.topk(num_hns)                                        \n",
    "        x_hn = x.index_select(0, idxs)                                             \n",
    "        y_hn = y.index_select(0, idxs)                                             \n",
    "        return nn.functional.nll_loss(x_hn, y_hn)\n",
    "    \n",
    "class CrossEntropy_OHEM(nn.CrossEntropyLoss):\n",
    "    \n",
    "    def __init__(self, ratio, *args, **kwargs):\n",
    "        super(CrossEntropy_OHEM, self).__init__(*args, **kwargs)                                 \n",
    "        self.nll_ohem = NLL_OHEM(ratio, *args, **kwargs)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        lsm = nn.functional.log_softmax(x, 1)\n",
    "        return self.nll_ohem(lsm, y)\n",
    "                                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25, unfreeze_epoch=0):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    history = {'loss': {'train': [], 'val': [], 'test': []},\n",
    "               'acc': {'train': [], 'val': [], 'test': []},}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        val_dset_iter = iter(dataloaders['val'])\n",
    "        test_dset_iter = iter(dataloaders['test'])\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        \n",
    "        model.train()  # Set model to training mode\n",
    "        \n",
    "        # unfreeze\n",
    "        if epoch == unfreeze_epoch:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "        \n",
    "\n",
    "        # Train loop\n",
    "        for iter_num, (inputs, labels) in enumerate(tqdm(dataloaders['train'])):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if iter_num % (dataset_sizes['train']//dataset_sizes['val'] + 1) == 0:\n",
    "                # forward\n",
    "                with torch.no_grad():\n",
    "                    inp_val, lbl_val = next(val_dset_iter)\n",
    "                    inp_val = inp_val.to(device)\n",
    "                    lbl_val = lbl_val.to(device)\n",
    "                    out_val = model(inp_val)\n",
    "                    loss_val = criterion(out_val, lbl_val)\n",
    "                    history['loss']['val'].append(loss_val.item())\n",
    "                    \n",
    "                    inp_test, lbl_test = next(test_dset_iter)\n",
    "                    inp_test = inp_test.to(device)\n",
    "                    lbl_test = lbl_test.to(device)\n",
    "                    out_test = model(inp_test)\n",
    "                    loss_test = criterion(out_test, lbl_test)\n",
    "                    history['loss']['test'].append(loss_test.item())\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # backward + optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # statistics\n",
    "            history['loss']['train'].append(loss.item())\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        model.eval()   # Set model to evaluate mode\n",
    "        \n",
    "        for mode in ['train', 'val', 'test']:\n",
    "            \n",
    "            running_corrects = 0\n",
    "            for inputs, labels in tqdm(dataloaders[mode]):\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # forward\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                \n",
    "            epoch_acc = (running_corrects.double() / dataset_sizes[mode]).item()\n",
    "            history['acc'][mode].append(epoch_acc)\n",
    "\n",
    "            print('{} Acc: {:.4f}'.format(mode, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if mode == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, history\n",
    "\n",
    "\n",
    "def eval_model(model, dataloader):\n",
    "    corrects=0\n",
    "    model.eval()\n",
    "    for inputs, labels in tqdm(dataloader):\n",
    "\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # forward\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_acc = (corrects.double() / len(dataloader.dataset)).item()\n",
    "    print(f'Test accuracy: {epoch_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgqMvL1r-DEN"
   },
   "outputs": [],
   "source": [
    "save_data = []\n",
    "\n",
    "for seed in [42069, 1337]:\n",
    "    seed_all(seed)\n",
    "    data = data_map('jpg')\n",
    "    train_map, val_map, test_map = train_val_test_split(data)\n",
    "    mean_std = get_mean_std(train_map)\n",
    "\n",
    "    data_transforms = {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_std['mean'], mean_std['std'])\n",
    "        ]),\n",
    "\n",
    "        'val': transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean_std['mean'], mean_std['std'])\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "\n",
    "    image_datasets = {'train': FlowerDSet(train_map, data_transforms['train']),\n",
    "                      'val': FlowerDSet(val_map, data_transforms['val']),\n",
    "                      'test': FlowerDSet(test_map, data_transforms['val'])}\n",
    "\n",
    "    dataloaders = {x: DataLoader(image_datasets[x], batch_size=64, shuffle=(True), num_workers=8)\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "\n",
    "    dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model_ft = models.resnet50(pretrained=True)#, aux_logits=False)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, 102)\n",
    "\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    # freeze layers\n",
    "    for param in model_ft.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    model_ft.fc.requires_grad_(True)\n",
    "\n",
    "    criterion = CrossEntropy_OHEM(0.3)\n",
    "\n",
    "    # Observe that all parameters are being optimized\n",
    "    optimizer_ft = optim.SGD(model_ft.parameters(), lr=1e-3, momentum=0.9)#, nesterov=True)\n",
    "\n",
    "    # Decay LR by a factor of 0.5 every 5 epochs\n",
    "    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer_ft, factor=0.5, patience=5,)\n",
    "    \n",
    "    # train\n",
    "    model_ft, hist = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=50, unfreeze_epoch=5)\n",
    "    model_ft.to(torch.device('cpu'))\n",
    "    \n",
    "    save_data.append((model_ft, hist))\n",
    "    \n",
    "torch.save(save_data, 'resnet-hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = torch.load('resnet-hist.zip')\n",
    "inc = torch.load('inception-hist.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x, f):\n",
    "    return np.convolve(x, np.ones(f)/f, mode='valid')\n",
    "\n",
    "def plot_acc(hist, model_name):\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for idx, data in enumerate(hist, 1):\n",
    "        tr = data['acc']['train']\n",
    "        vl = data['acc']['val']\n",
    "        ts = data['acc']['test']\n",
    "        \n",
    "        plt.plot(tr, label=f'Train Accuracy {model_name} #{idx}')\n",
    "        plt.plot(vl, label=f'Validation Accuracy {model_name} #{idx}')        \n",
    "        plt.plot(ts, label=f'Test Accuracy {model_name} #{idx}')\n",
    "        \n",
    "        ep = np.argmax(vl)\n",
    "        print(f\"{model_name} test acc. at epoch {ep}: {ts[ep]:.4} (best epoch on val)\")\n",
    "        \n",
    "    plt.xlabel(\"Epoch number\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(f\"{model_name} Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    \n",
    "def plot_losses(hist, model_name):\n",
    "    w=10\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for idx, data in enumerate(hist, 1):\n",
    "        tr = smooth(data['loss']['train'], w)\n",
    "        vl = smooth(np.repeat(data['loss']['val'], 3), w)\n",
    "        ts = smooth(np.repeat(data['loss']['test'], 3), w)\n",
    "        \n",
    "        plt.plot(tr, label=f'Train Loss {model_name} #{idx}')\n",
    "        plt.plot(vl, label=f'Validation Loss {model_name} #{idx}')        \n",
    "        plt.plot(ts, label=f'Test Loss {model_name} #{idx}')\n",
    "        \n",
    "    \n",
    "    plt.xlabel(\"Iteration number\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(f\"{model_name} Loss\")\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(res, 'Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(res, 'Resnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(inc, 'Inception v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acc(inc, 'Inception v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A3-Transfer_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
